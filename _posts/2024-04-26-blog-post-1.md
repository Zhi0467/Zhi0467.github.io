---
title: 'What is the architecture of an ideal agent?'
date: 2025-11-22
permalink: /posts/2025/01/blog-post-1/
tags:
  - meta-thinking
---

Here is a beta version of an ideal agent that I have been thinking about, this helps me personally to categorize a large amout of new ML research paper to one of the boxes or arrows, and to identify gaps.

- RLVR sits on the arrow from Episodic memory to Reasoning model
- Autoregressive sampling, speculative decoding, vLLM, any inference engine sit on the arrow from Reasoning model to Inference, two recent work on this arrow include TiDAR from Nvidia and power sampling from Harvard.
- The /compact command in Claude code sits on the arrow from Inference to Reasoning model.
- World model is largely missing, as the mainstream way to go from Perception to Inference is via Reasoning in language, but a world model with instinct should be a parallel channel.
- Control is pure engineering, protocals, and infrasture.
- Memory and context management is crucial for allowing the agent to run for a long time. This is a major gap.

![ideal agent](/images/ideal-agent.png)

For example:
an instantiation of an agent, say Gemini 3:

- perception module: text, image, PDF, audio
- reasoning model: parameterized by a decoder-only LLM, with architecture design like MoE, a reasoning and planning layer etc, but overall transformer based.
- inference: autoregressive machine parameterized by the same decoder-only LLM to generate, various techniques, parallel thinking, CoT, etc
- context management: unknown
- world model: lacking, no instinct module or 3D predictive module built in as a parallel stream of inference
- control: can interact with UI once inference is done (or along with inference, streaming rendering), can interact with Google search API, can interact with VM code sandbox internally hosted by Google, can interact with IDE API such Antigravity
    - not sure if MCP is used, but function calling, yes.
    - robotics control is used in Gemini Robotics, with different upstream modules.
- episodic memory: the context window with various caching.
- feedback: non-existent as the model is frozen, no episodic memory, but in training, feedback exists in post training stage, where episodic memory is the whole inference + control history, then used to perform RLVR on the reasoning model
- long-term memory: exist, but very crude, basically a json file that tracks some facts about the user, specific to an assistant use scenario
- continual learning: doesnâ€™t exist
- conditioned input from memory: exists in the crude way as how long-term memory is set up
